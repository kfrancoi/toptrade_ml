# toptrade

This project was automaticly generated by `template-ml` [read the docs](https://github.com/Sagacify/template-ml)

# Build

```sh
docker-compose build notebooks
docker-compose build gpu
docker-compose build api
```

**Note** To build the api you need to docker login first with your docker hub cred

# Test (api only)

```sh
docker-compose run -e RUN_ENV=test api pytest
```

# Notebook

Start notebooks on port 8888

```sh
docker-compose up notebooks
```

Access your notebooks at : jean-jacques.sagacify.com:8888

# Data

Don't forged to read the doc about [data management](https://github.com/Sagacify/template-ml#data-management) and [commiting your work](https://github.com/Sagacify/template-ml#commit-your-work).

This will fetch your raw, processed and resources data on S3.
Options:

- `process`: Download raw data and process them instead of only downloading processed data
- `upload`: Will upload the newly generated processed data (only work with `-p`)

```sh
docker-compose run notebooks papermill notebooks/data_management.ipynb notebooks/data_management_out.ipynb --log-output (-p process True) (-p upload True)
```

# transfomer

Don't forged to read the doc about [model development](https://github.com/Sagacify/template-ml#model-development).

## Train

Train your model following the configuration (`config`) provided

### CPU training
```sh
docker-compose run --user $(id -u):$(id -g) notebooks papermill notebooks/pipeline_transformer.ipynb notebooks/pipeline_transformer_out.ipynb --log-output (-p config config/transformer.json)
```

### GPU training
```sh
docker-compose run -e CUDA_VISIBLE_DEVICES=0 --user $(id -u):$(id -g) gpu papermill notebooks/pipeline_transformer.ipynb notebooks/pipeline_transformer_out.ipynb --log-output (-p config config/transformer.json)
```


## Prod

Don't forged to read the doc about the [predictor](https://github.com/Sagacify/template-ml#predictor) and [commiting your work](https://github.com/Sagacify/template-ml#commit-your-work).

In order to create a production docker image containing a Flask API serving your model we will extends the image https://github.com/Sagacify/saga-predictor-base-image :


1. Put your model files (md5, pickle, json) and report on `/model`
2. Update requirements-api.txt with your requirements
3. Implement your Predictor in `resources/predictor.py`. This class must implements:
  - `__init__` called once on the server initialization with a single argument being the model path (used to load your md5 model, etc)
  - `get_report` returning your model report (a dictionary)
  - `predict` called at every prediction with a single argument being a dictionary containing the input data
4. Test your implementation using the `api` service in `docker-compose.yml` (Don't forget to login using `docker login` with your docker hub cred) Test this api locally
5. **First time**: Setup your project on CircleCI. Add env variable for AWS
6. Upload your model on S3 at `s3://toptrade-bucket/models/transformer_latest`
7. Create a Git branch matching `/.*model-prod.*/` commit and push your change on it (it will trigger the CircleCI Build)
8. Create a pull request and merge on master when reviewed. It should upload a version of your predictor api on dockerHub.
9. Update or create new service in docker-compose to test your newly generated image
